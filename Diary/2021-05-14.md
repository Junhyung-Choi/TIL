# 2021-05-14

- What I learned:
    - Linear Algebra
        - Orthogonality and Least Squares
            - Orthogonal Projection

            $$cu = \hat y \\
            \hat{y} = \text{proj}_{L}y = {y \cdot u \over u \cdot u}u$$

            $$\hat{y} \text{ is in } W \\
            z \text{ is in } W^{\bot} \\
            y = \hat{y} + z$$

            $$\hat{y} = c_1u_1 + \cdots + c_pu_p \\
            c_j = {y \cdot u_j \over u_j \cdot u_j}$$

            - The Orthogonal Decomposition Theorem
                - Let W be a subspace of $R^n$. Then each y in $R^n$ can be written uniquely in the form $y = \hat{y} + z$ where $\hat{y}$ is in W and z is in $W^{\bot}$
            - The Best Approximation Theorem
                - Let W be a subsapce of $R^n$. y any vecotr in $R^n$, and $\hat {y}$  the orthogonal projection of y onto W. Then $\hat {y}$  is the closest point in W to y, in the sense that

                    $$\lVert y- \hat{y} \rVert < \lVert y- v \rVert$$

                    for all v in W distinct from $\hat {y}$ .

            - If $\left\{ u_1,\cdots,u_p \right\}$ is an **orthonormal basis** for a subspace W of $R^n$, then

                $$\text{proj}_{W}y = (y \cdot u_1)u_1 + (y \cdot u_2)u_2 + \cdots +(y \cdot u_p)u_p $$

            - If U = $[u_1 \space u_2 \space \cdots u_p]$,then

                $$\text{proj}_{W}y = U^{\bot}Uy$$

- What was interesting:
    - Learned about Matrix Vector product
    - Learned about Notion
- What I regret:
    - Didn't